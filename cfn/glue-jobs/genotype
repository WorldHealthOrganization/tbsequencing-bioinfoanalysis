import sys
# import boto3
# import os
# import json

from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

from awsglue.dynamicframe import DynamicFrame

import pyspark.sql.functions as F

## @params: [JOB_NAME database_name]
args = getResolvedOptions(
    sys.argv,
    ['JOB_NAME', "glue_db_name", "postgres_db_name"]
)

glueContext = GlueContext(SparkContext().getOrCreate())
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

dbname = args["postgres_db_name"]
glue_dbname = args["glue_db_name"]

data_frame = {}
tables = { 
        "public": {
            "submission" : [
                "sample",
                "summarysequencingstats",
                "taxonomystats"
            ]
        }
}

for schema in tables.keys():
    if isinstance(tables[schema], list):
        for table in tables[schema]:
            tname = dbname + "_biosql_" + table
            data_frame[table] = glueContext.create_data_frame.from_catalog(
                database = glue_dbname,
                table_name = tname
            )
    elif isinstance(tables[schema], dict):
        for app in tables[schema].keys():
            for table in tables[schema][app]:
                tname = dbname + "_" + schema + "_" + app + "_" + table
                data_frame[table] = glueContext.create_data_frame.from_catalog(
                    database = glue_dbname,
                    table_name = tname
                )
    else: 
        raise ValueError
    
# Adding extra safeguard
# Filter sample ids that have been correctly processed
# bioanalysis_status = 'Uploaded to S3'
samples = (
    data_frame["sample"]
    .where(
        F.col("bioanalysis_status")=="Uploaded to S3"
    )
    .withColumnRenamed(
        "id",
        "sample_id"
    )
)

contaminated = (
    data_frame["summarysequencingstats"]
    .join(
        data_frame["taxonomystats"],
        on="sample_id",
        how="inner"
    )
    .where(
        (F.col("ncbi_taxon_id")==1773)
        & (F.col("value")<1)
        & (F.col("percentage_of_properly_paired_reads")<20)
        & (F.col("median_depth")<10)
    )
    .select(
        F.col("sample_id")
    )
)

print(contaminated.count())

datasource0 = (
    glueContext.create_dynamic_frame.from_catalog(
        database = args["glue_db_name"],
        table_name = "genotype",
        transformation_ctx = "datasource0"
    )
    .toDF()
    .join(
        samples,
        on="sample_id",
        how="inner"
    )
    .join(
        contaminated,
        on="sample_id",
        how="left_anti"
    )
)

datasource1 = (
    DynamicFrame
    .fromDF(
        datasource0,
        glueContext
    )
)

applymapping1 = ApplyMapping.apply(frame = datasource1, mappings = [("genotyper", "string", "genotyper", "string"), ("sampleid", "long", "sample_id", "int"), ("chrom", "string", "chromosome", "string"), ("pos", "long", "position", "int"), ("altad", "long", "alternative_ad", "int"), ("ref", "string", "reference_nucleotide", "string"), ("alt", "string", "alternative_nucleotide", "string"), ("qual", "string", "quality", "double"), ("refad", "long", "reference_ad", "int"), ("id", "string", "variant_id", "int"), ("totaldp", "long", "total_dp", "int"), ("value", "string", "genotype_value", "string")], transformation_ctx = "applymapping1")

selectfields2 = SelectFields.apply(frame = applymapping1, paths = ["sample_id", "position", "total_dp", "quality", "chromosome", "alternative_ad", "alternative_nucleotide", "reference_ad", "genotyper", "reference_nucleotide", "genotype_value"], transformation_ctx = "selectfields2")

resolvechoice3 = ResolveChoice.apply(frame = selectfields2, choice = "MATCH_CATALOG", database = args["glue_db_name"], table_name = f"{args['postgres_db_name']}_genphensql_staged_genotype", transformation_ctx = "resolvechoice3")

resolvechoice4 = ResolveChoice.apply(frame = resolvechoice3, choice = "make_cols", transformation_ctx = "resolvechoice4")

datasink5 = glueContext.write_dynamic_frame.from_catalog(frame = resolvechoice4, database = args["glue_db_name"], table_name = f"{args['postgres_db_name']}_genphensql_staged_genotype", transformation_ctx = "datasink5")

job.commit()